# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
from numpy.linalg import pinv
import numpy as np
import pandas as pd


# import jax

np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    self.pinx = None
  

  def fit_sklearn_LR(self,X,y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    pass
  
  def fit_normal_equations(self, X1, y1):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    X = X1[:]
    y = y1[:]

    if(self.fit_intercept):
        X.insert(0, "intercept", 1)
    
    X = np.array(X)
    y = np.array(y)

    XT = np.transpose(X)
    XTX_inv = np.linalg.pinv(np.matmul(XT,X))
    K = np.matmul(XT,y)
    self.coef_ = np.matmul(XTX_inv,K)

  def fit_SVD(self,X1,y1):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    X = X1[:]
    y = y1[:]

    if(self.fit_intercept):
        X.insert(0, "intercept", 1)
    
    X = np.array(X)
    y = np.array(y)
    self.coef_ = pinv(X).dot(y)

  def mse_loss(self, y_hat, y):                
    # Compute the MSE loss with the learned model
    pass


  def compute_gradient(self, X1, y1, penalty, c):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    assert (penalty == 'unregularized' or penalty == 'l2') 
    X = X1[:]
    y = y1[:] 
    
    if(self.fit_intercept):
      X.insert(0, "intercept", 1)

    X = np.array(X)
    y = np.array(y)
    y = np.reshape(y, (len(y), 1))
    self.coef_ = np.reshape(self.coef_, (len(self.coef_), 1))   
    m = len(X)

    if penalty == 'unregularized':
      gradient = 2/m * np.matmul(X.T,(np.matmul(X,self.coef_) - y))
      grad = []
      for i in range(len(gradient)):
        grad.append(gradient[i][0])
      return grad
    
    else:
      gradient = 2/m * np.matmul(X.T,(np.matmul(X,self.coef_) - y)) + (2* c) * self.coef_
      grad = []
      for i in range(len(gradient)):
        grad.append(gradient[i][0])
      return grad
      

  def _gradient(self, X1, y1, penalty, iterations, alpha):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    X = X1[:]
    y = y1[:] 
    
    if(self.fit_intercept):
      X.insert(0, "intercept", 1)
      
    coef = np.random.randn(X.shape[1], 1)
    X = np.array(X)
    y = np.array(y)
    y = np.reshape(y, (len(y), 1))        
    

    cost_lst = []
    
    m = len(X)
    for i in range(iterations):        
      gradients = 2/m * np.matmul(X.T,(np.matmul(X,coef) - y))
      coef = coef - alpha * gradients     
      y_hat = X.dot(coef)      
      cost_value = 1/(2*len(y))*((y_hat - y)**2) #Calculate the loss for each training instance
      total = 0
      for i in range(len(y)):
          total += cost_value[i][0] #Calculate the cost function for each iteration
      cost_lst.append(total)
    self.coef_ = coef

  def compute_jax_gradient(self):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    pass

  def fit_gradient_descent(self, batch_size, gradient_type, penalty_type, num_iters=20, lr=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    pass



  def fit_SGD_with_momentum(self, penalty='l2', beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    pass

  def predict_gd(self, X1):
    # Funtion to run the LinearRegression on a test data point
    X = X1[:]
    if(self.fit_intercept and "intercept" not in X.columns):
        X.insert(0, "intercept", 1)
    X = np.array(X)
    y_hat = np.matmul(X,self.coef_)
    y_h = []
    for i in range(len(y_hat)):
      y_h.append(y_hat[i][0])
    return pd.Series(y_h)
  
  def predict(self, X1):
    X = X1[:]
    if(self.fit_intercept and "intercept" not in X.columns):
      X.insert(0, "intercept", 1)
    X = np.array(X)
    y_hat = np.matmul(X,self.coef_)
    return pd.Series(y_hat) 
  
  
  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    pass

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    pass


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    pass